{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"After a five year struggle, creditors of the collapsed, fraud-ridden BCCI will receive a payment of $2.65 billion on Tuesday, equal to 24.5 percent of their claims, a spokesman for the liquidators said on Monday.\\nBank of Credit and Commerce International, founded in 1972, was closed by central banks in 1991 and collapsed with debts of more than $12 billion when evidence of massive fraud and money laundering was unearthed leading to a tangled web of litigation which shows no sign of reaching an early conclusion.\\nBCCI had assets of $24 billion and operations in 71 countries at the time of its collapse.\\nLiquidator Deloitte and Touche said a further payment, reportedly of 10 percent, of the admitted claims which total some $10.5 billion should be made in the next 12 to 16 months.\\nThe gross fund of amounts recovered by the liquidators stands at around $4.0 billion and includes $1.5 billion paid by BCCI's majority shareholder, the government of Abu Dhabi, which will pay a further $250 million in due course following a settlement reached earlier this year.\\nThis, in addition to efforts by US authorities which resulted in the recovery of more than $500 million from the United States paved the way for the first dividend payment.\\nA further $245 million was paid by Saudi billionaire Sheikh Khalid bin Mahfouz who the liquidators alleged was involved in covering up the BCCI scandal. Under a 1995 Luxembourg court settlement, Mahfouz agreed to pay without admitting liability, in return for the lawsuits being dropped.\\nThe liquidators still have outstanding claims against the Bank of England, the Institut Monetaire Luxembourgeois (BCCI's operations were based in Luxembourgh) and the auditors of the bank, international accountancy firms Price Waterhouse and Ernst &amp; Whinney, now part of the merged Ernst &amp; Young.\\nPrice Waterhouse has said it is making a multi-billion dollar counter claim against Abu Dhabi.\\nFurther law suits are also pending around the world in an attempt to recover further amounts.\\nThe two biggest groups of creditors of BCCI are in the United Arab Emirates (UAE) and in Britain.\\nThe English liquidators of BCCI, Deloite &amp; Touche, have recovered over $1 billion and have been paid a massive $200 million in fees.\\nIn a report to the High Court earlier this year, the liquidator said legal fees in the liquidation ammounted to over $75 million so far.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "Text = open('../data/C50train/JoeOrtiz/242939newsML.txt').read()\n",
    "Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "##  Mini-Challenge - 1 \n",
    "***\n",
    "### Instructions\n",
    "* Perform a sentence tokenization on the above data using `sent_tokenize()` and store it in a variable called '**Sent**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After a five year struggle, creditors of the collapsed, fraud-ridden BCCI will receive a payment of $2.65 billion on Tuesday, equal to 24.5 percent of their claims, a spokesman for the liquidators said on Monday.',\n",
       " 'Bank of Credit and Commerce International, founded in 1972, was closed by central banks in 1991 and collapsed with debts of more than $12 billion when evidence of massive fraud and money laundering was unearthed leading to a tangled web of litigation which shows no sign of reaching an early conclusion.',\n",
       " 'BCCI had assets of $24 billion and operations in 71 countries at the time of its collapse.',\n",
       " 'Liquidator Deloitte and Touche said a further payment, reportedly of 10 percent, of the admitted claims which total some $10.5 billion should be made in the next 12 to 16 months.',\n",
       " \"The gross fund of amounts recovered by the liquidators stands at around $4.0 billion and includes $1.5 billion paid by BCCI's majority shareholder, the government of Abu Dhabi, which will pay a further $250 million in due course following a settlement reached earlier this year.\",\n",
       " 'This, in addition to efforts by US authorities which resulted in the recovery of more than $500 million from the United States paved the way for the first dividend payment.',\n",
       " 'A further $245 million was paid by Saudi billionaire Sheikh Khalid bin Mahfouz who the liquidators alleged was involved in covering up the BCCI scandal.',\n",
       " 'Under a 1995 Luxembourg court settlement, Mahfouz agreed to pay without admitting liability, in return for the lawsuits being dropped.',\n",
       " \"The liquidators still have outstanding claims against the Bank of England, the Institut Monetaire Luxembourgeois (BCCI's operations were based in Luxembourgh) and the auditors of the bank, international accountancy firms Price Waterhouse and Ernst &amp; Whinney, now part of the merged Ernst &amp; Young.\",\n",
       " 'Price Waterhouse has said it is making a multi-billion dollar counter claim against Abu Dhabi.',\n",
       " 'Further law suits are also pending around the world in an attempt to recover further amounts.',\n",
       " 'The two biggest groups of creditors of BCCI are in the United Arab Emirates (UAE) and in Britain.',\n",
       " 'The English liquidators of BCCI, Deloite &amp; Touche, have recovered over $1 billion and have been paid a massive $200 million in fees.',\n",
       " 'In a report to the High Court earlier this year, the liquidator said legal fees in the liquidation ammounted to over $75 million so far.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "Sent = nltk.sent_tokenize(Text)\n",
    "Sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "##  Mini-Challenge - 2 \n",
    "***\n",
    "**Bag of Words** <br>\n",
    "In this task, we will try to do the basic NLP operations of tokenizing, removing stop words and lemmatizing on our data. We will also try to create a list of most frequent words\n",
    "### Instructions\n",
    "- Iterate over every Sentence in the list **Sent**  using a for loop and convert every sentence into \n",
    "    - lower case \n",
    "    - and then tokenize it using the instantiated object \n",
    "- Now remove the stopwords from the tokens \n",
    "- Lemmatize them using `WordNetLemmatizer()` and save it in `lemmatized_tokens`\n",
    "- Append `lemmatized_tokens` into the list called **Text**\n",
    "- Convert `Counter(lemmatized_tokens)` into dictionary and save it in a variable called `BoW_dict`.\n",
    "- Sort `BoW_dict` in descending order using `sorted()` function with the parameters `BoW_dict.items()`, `key=operator.itemgetter(1)`, `reverse=True`. Store it in a variable called `sorted_d`\n",
    "- Finally append them into the list called **Texts** \n",
    "- Print `Texts` to check out the list of words with their frequency in descending order.\n",
    "- Print Top 10 words from the `Texts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('five', 1), ('year', 1), ('struggle', 1), ('creditor', 1), ('collapsed', 1), ('fraud', 1), ('ridden', 1), ('bcci', 1), ('receive', 1), ('payment', 1), ('2', 1), ('65', 1), ('billion', 1), ('tuesday', 1), ('equal', 1), ('24', 1), ('5', 1), ('percent', 1), ('claim', 1), ('spokesman', 1), ('liquidator', 1), ('said', 1), ('monday', 1)], [('bank', 2), ('credit', 1), ('commerce', 1), ('international', 1), ('founded', 1), ('1972', 1), ('closed', 1), ('central', 1), ('1991', 1), ('collapsed', 1), ('debt', 1), ('12', 1), ('billion', 1), ('evidence', 1), ('massive', 1), ('fraud', 1), ('money', 1), ('laundering', 1), ('unearthed', 1), ('leading', 1), ('tangled', 1), ('web', 1), ('litigation', 1), ('show', 1), ('sign', 1), ('reaching', 1), ('early', 1), ('conclusion', 1)], [('bcci', 1), ('asset', 1), ('24', 1), ('billion', 1), ('operation', 1), ('71', 1), ('country', 1), ('time', 1), ('collapse', 1)], [('10', 2), ('liquidator', 1), ('deloitte', 1), ('touche', 1), ('said', 1), ('payment', 1), ('reportedly', 1), ('percent', 1), ('admitted', 1), ('claim', 1), ('total', 1), ('5', 1), ('billion', 1), ('made', 1), ('next', 1), ('12', 1), ('16', 1), ('month', 1)], [('billion', 2), ('gross', 1), ('fund', 1), ('amount', 1), ('recovered', 1), ('liquidator', 1), ('stand', 1), ('around', 1), ('4', 1), ('0', 1), ('includes', 1), ('1', 1), ('5', 1), ('paid', 1), ('bcci', 1), ('majority', 1), ('shareholder', 1), ('government', 1), ('abu', 1), ('dhabi', 1), ('pay', 1), ('250', 1), ('million', 1), ('due', 1), ('course', 1), ('following', 1), ('settlement', 1), ('reached', 1), ('earlier', 1), ('year', 1)], [('addition', 1), ('effort', 1), ('u', 1), ('authority', 1), ('resulted', 1), ('recovery', 1), ('500', 1), ('million', 1), ('united', 1), ('state', 1), ('paved', 1), ('way', 1), ('first', 1), ('dividend', 1), ('payment', 1)], [('245', 1), ('million', 1), ('paid', 1), ('saudi', 1), ('billionaire', 1), ('sheikh', 1), ('khalid', 1), ('bin', 1), ('mahfouz', 1), ('liquidator', 1), ('alleged', 1), ('involved', 1), ('covering', 1), ('bcci', 1), ('scandal', 1)], [('1995', 1), ('luxembourg', 1), ('court', 1), ('settlement', 1), ('mahfouz', 1), ('agreed', 1), ('pay', 1), ('without', 1), ('admitting', 1), ('liability', 1), ('return', 1), ('lawsuit', 1), ('dropped', 1)], [('bank', 2), ('ernst', 2), ('amp', 2), ('liquidator', 1), ('still', 1), ('outstanding', 1), ('claim', 1), ('england', 1), ('institut', 1), ('monetaire', 1), ('luxembourgeois', 1), ('bcci', 1), ('operation', 1), ('based', 1), ('luxembourgh', 1), ('auditor', 1), ('international', 1), ('accountancy', 1), ('firm', 1), ('price', 1), ('waterhouse', 1), ('whinney', 1), ('part', 1), ('merged', 1), ('young', 1)], [('price', 1), ('waterhouse', 1), ('said', 1), ('making', 1), ('multi', 1), ('billion', 1), ('dollar', 1), ('counter', 1), ('claim', 1), ('abu', 1), ('dhabi', 1)], [('law', 1), ('suit', 1), ('also', 1), ('pending', 1), ('around', 1), ('world', 1), ('attempt', 1), ('recover', 1), ('amount', 1)], [('two', 1), ('biggest', 1), ('group', 1), ('creditor', 1), ('bcci', 1), ('united', 1), ('arab', 1), ('emirate', 1), ('uae', 1), ('britain', 1)], [('english', 1), ('liquidator', 1), ('bcci', 1), ('deloite', 1), ('amp', 1), ('touche', 1), ('recovered', 1), ('1', 1), ('billion', 1), ('paid', 1), ('massive', 1), ('200', 1), ('million', 1), ('fee', 1)], [('report', 1), ('high', 1), ('court', 1), ('earlier', 1), ('year', 1), ('liquidator', 1), ('said', 1), ('legal', 1), ('fee', 1), ('liquidation', 1), ('ammounted', 1), ('75', 1), ('million', 1), ('far', 1)]]\n",
      "\n",
      "Top 10 words:\n",
      " [('report', 1), ('high', 1), ('court', 1), ('earlier', 1), ('year', 1), ('liquidator', 1), ('said', 1), ('legal', 1), ('fee', 1), ('liquidation', 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "en_stop = set(stopwords.words('english'))\n",
    "\n",
    "Lema = WordNetLemmatizer()\n",
    "\n",
    "Texts = []\n",
    "Text = []\n",
    "for i in Sent:\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    lemmatized_tokens = [Lema.lemmatize(i) for i in stopped_tokens]\n",
    "    Text.append(lemmatized_tokens)\n",
    "    BoW_dict = dict(Counter(lemmatized_tokens))\n",
    "    sorted_d = sorted(BoW_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    Texts.append(sorted_d)\n",
    "print(Texts) \n",
    "print(\"\\nTop 10 words:\\n\", sorted_d[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "##  Mini-Challenge - 3\n",
    "***\n",
    "Since Nouns are important in Topic Modeling process, we will try to figure out the top nouns from the bag of words we created in the last task.\n",
    "### Instructions\n",
    "- Join the previously created bag of words `lemmatized_tokens` back into a string using `join()` method and store the result in `BoW_joined`.\n",
    "- Convert `Bow_joined` into a textblob using `TextBlob()` method and store the result into `blob`.\n",
    "- Print out the `blob.tags` to look at the different tags associated with the words.\n",
    "- Get the tags of all the words from `lemmatized_tokens` using `blob.tags` and store the result in a variable called `tags`\n",
    "- From `tags`, extract the words which have `NN` tags and store them to a list called `nouns`\n",
    "- The top 10 words which have appeared most frequently are already stored into a list called `top_words`\n",
    "- Compare the two lists `top_words` and `nouns` and store the common elements between them in a new list called `top_nouns`\n",
    "- Print `top_nouns` to see most commonly appearing nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 tags:\n",
      " [('report', 'NN'), ('high', 'JJ'), ('court', 'NN'), ('earlier', 'RBR'), ('year', 'NN'), ('liquidator', 'NN'), ('said', 'VBD'), ('legal', 'JJ'), ('fee', 'NN'), ('liquidation', 'NN')]\n",
      "\n",
      "Top Nouns: ['liquidator', 'court', 'report', 'year', 'liquidation', 'fee']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  \n",
    "\n",
    "#Extracting the top 10 words with their count\n",
    "top_10 = sorted_d[:10]\n",
    "\n",
    "#Storing only the top 10 words\n",
    "top_words=[]\n",
    "for x in top_10:\n",
    "    top_words.append(x[0])\n",
    "\n",
    "\n",
    "#Code begins\n",
    "\n",
    "# Joining the list back to sentences\n",
    "BoW_joined = \" \".join(lemmatized_tokens)\n",
    "\n",
    "# Converting the data to textblob\n",
    "blob = TextBlob(BoW_joined)\n",
    "\n",
    "# Print the first 10 tags\n",
    "print(\"First 10 tags:\\n\" ,blob.tags[:10])\n",
    "\n",
    "#Extracting the tags\n",
    "tags = blob.tags\n",
    "\n",
    "#Initialising an empty list\n",
    "nouns = []\n",
    "\n",
    "#Extracing the words with NN tags\n",
    "for x in tags:\n",
    "    if x[1]==\"NN\":\n",
    "        nouns.append(x[0])\n",
    "        \n",
    "\n",
    "#Comparing the two lists to extract the common elements        \n",
    "top_nouns=[x for x in nouns if x in top_words]\n",
    "top_nouns  = list(set(top_nouns))\n",
    "\n",
    "print(\"\\nTop Nouns:\", top_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "##  Mini-Challenge - 4\n",
    "***\n",
    "Using the method `.Dictionary()` inside the module `corpora` to create a unique token for every word and also print out the tokens assigned respectively using the `.token2id` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(171 unique tokens: ['2', '24', '5', '65', 'bcci']...)\n",
      "{'2': 0, '24': 1, '5': 2, '65': 3, 'bcci': 4, 'billion': 5, 'claim': 6, 'collapsed': 7, 'creditor': 8, 'equal': 9, 'five': 10, 'fraud': 11, 'liquidator': 12, 'monday': 13, 'payment': 14, 'percent': 15, 'receive': 16, 'ridden': 17, 'said': 18, 'spokesman': 19, 'struggle': 20, 'tuesday': 21, 'year': 22, '12': 23, '1972': 24, '1991': 25, 'bank': 26, 'central': 27, 'closed': 28, 'commerce': 29, 'conclusion': 30, 'credit': 31, 'debt': 32, 'early': 33, 'evidence': 34, 'founded': 35, 'international': 36, 'laundering': 37, 'leading': 38, 'litigation': 39, 'massive': 40, 'money': 41, 'reaching': 42, 'show': 43, 'sign': 44, 'tangled': 45, 'unearthed': 46, 'web': 47, '71': 48, 'asset': 49, 'collapse': 50, 'country': 51, 'operation': 52, 'time': 53, '10': 54, '16': 55, 'admitted': 56, 'deloitte': 57, 'made': 58, 'month': 59, 'next': 60, 'reportedly': 61, 'total': 62, 'touche': 63, '0': 64, '1': 65, '250': 66, '4': 67, 'abu': 68, 'amount': 69, 'around': 70, 'course': 71, 'dhabi': 72, 'due': 73, 'earlier': 74, 'following': 75, 'fund': 76, 'government': 77, 'gross': 78, 'includes': 79, 'majority': 80, 'million': 81, 'paid': 82, 'pay': 83, 'reached': 84, 'recovered': 85, 'settlement': 86, 'shareholder': 87, 'stand': 88, '500': 89, 'addition': 90, 'authority': 91, 'dividend': 92, 'effort': 93, 'first': 94, 'paved': 95, 'recovery': 96, 'resulted': 97, 'state': 98, 'u': 99, 'united': 100, 'way': 101, '245': 102, 'alleged': 103, 'billionaire': 104, 'bin': 105, 'covering': 106, 'involved': 107, 'khalid': 108, 'mahfouz': 109, 'saudi': 110, 'scandal': 111, 'sheikh': 112, '1995': 113, 'admitting': 114, 'agreed': 115, 'court': 116, 'dropped': 117, 'lawsuit': 118, 'liability': 119, 'luxembourg': 120, 'return': 121, 'without': 122, 'accountancy': 123, 'amp': 124, 'auditor': 125, 'based': 126, 'england': 127, 'ernst': 128, 'firm': 129, 'institut': 130, 'luxembourgeois': 131, 'luxembourgh': 132, 'merged': 133, 'monetaire': 134, 'outstanding': 135, 'part': 136, 'price': 137, 'still': 138, 'waterhouse': 139, 'whinney': 140, 'young': 141, 'counter': 142, 'dollar': 143, 'making': 144, 'multi': 145, 'also': 146, 'attempt': 147, 'law': 148, 'pending': 149, 'recover': 150, 'suit': 151, 'world': 152, 'arab': 153, 'biggest': 154, 'britain': 155, 'emirate': 156, 'group': 157, 'two': 158, 'uae': 159, '200': 160, 'deloite': 161, 'english': 162, 'fee': 163, '75': 164, 'ammounted': 165, 'far': 166, 'high': 167, 'legal': 168, 'liquidation': 169, 'report': 170}\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(Text)\n",
    "print(dictionary)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "##  Mini-Challenge - 5\n",
    "***\n",
    "Now convert the dictionary into a bag of words list using the `.doc2bow()` method in `dictionary` and store it in a variable **corpus** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1)]\n",
      "[(5, 1), (7, 1), (11, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1)]\n",
      "[(1, 1), (4, 1), (5, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1)]\n",
      "[(2, 1), (5, 1), (6, 1), (12, 1), (14, 1), (15, 1), (18, 1), (23, 1), (54, 2), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1)]\n",
      "[(2, 1), (4, 1), (5, 2), (12, 1), (22, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1)]\n",
      "[(14, 1), (81, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1)]\n",
      "[(4, 1), (12, 1), (81, 1), (82, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1)]\n",
      "[(83, 1), (86, 1), (109, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1)]\n",
      "[(4, 1), (6, 1), (12, 1), (26, 2), (36, 1), (52, 1), (123, 1), (124, 2), (125, 1), (126, 1), (127, 1), (128, 2), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1)]\n",
      "[(5, 1), (6, 1), (18, 1), (68, 1), (72, 1), (137, 1), (139, 1), (142, 1), (143, 1), (144, 1), (145, 1)]\n",
      "[(69, 1), (70, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1)]\n",
      "[(4, 1), (8, 1), (100, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1)]\n",
      "[(4, 1), (5, 1), (12, 1), (40, 1), (63, 1), (65, 1), (81, 1), (82, 1), (85, 1), (124, 1), (160, 1), (161, 1), (162, 1), (163, 1)]\n",
      "[(12, 1), (18, 1), (22, 1), (74, 1), (81, 1), (116, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in Text]\n",
    "for line in corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/ppt-icons.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "##  Mini-Challenge - 6\n",
    "***\n",
    "Create an LDA model with number of topics as 5 of your choice and your choice of total passes. Now print out the top 5 topics and also the top 3 words in every topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=171, num_topics=5, decay=0.5, chunksize=2000)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary, passes=20)\n",
    "\n",
    "print(ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.035*\"bank\" + 0.019*\"billion\" + 0.019*\"international\"')\n",
      "(1, '0.033*\"bcci\" + 0.018*\"amp\" + 0.018*\"fee\"')\n",
      "(2, '0.028*\"billion\" + 0.028*\"said\" + 0.028*\"claim\"')\n",
      "(3, '0.032*\"liquidator\" + 0.025*\"billion\" + 0.025*\"bcci\"')\n",
      "(4, '0.017*\"court\" + 0.017*\"payment\" + 0.017*\"united\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in ldamodel.print_topics(num_topics=5, num_words=3):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/icon/quiz.png\" alt=\"Concept-Alert\" style=\"width: 100px;float:left; margin-right:15px\"/>\n",
    "<br /> \n",
    "\n",
    "## Topic Modelling\n",
    "***\n",
    "\n",
    "Q1. What percentage of the total statements are correct with regards to Topic Modeling?\n",
    "```python\n",
    "1. It is a supervised learning technique\n",
    "2. LDA (Linear Discriminant Analysis) can be used to perform topic modeling\n",
    "3. Selection of number of topics in a model does not depend on the size of data\n",
    "4. Number of topic terms are directly proportional to size of the data\n",
    "A) 0\n",
    "B) 25\n",
    "C) 50\n",
    "D) 75\n",
    "E) 100\n",
    "\n",
    "Solution: (A)\n",
    "\n",
    "LDA is unsupervised learning model, LDA is latent Dirichlet allocation, not Linear discriminant analysis. Selection of the number of topics is directly proportional to the size of the data, while number of topic terms is not directly proportional to the size of the data. Hence none of the statements are correct.\n",
    "```\n",
    "\n",
    "Q2. In Latent Dirichlet Allocation model for text classification purposes, what does alpha and beta hyperparameter represent-\n",
    "```python\n",
    "A) Alpha: number of topics within documents, beta: number of terms within topics False\n",
    "B) Alpha: density of terms generated within topics, beta: density of topics generated within terms False\n",
    "C) Alpha: number of topics within documents, beta: number of terms within topics False\n",
    "D) Alpha: density of topics generated within documents, beta: density of terms generated within topics True\n",
    "\n",
    "Solution: (D)\n",
    "```\n",
    "Q3. Social Media platforms are the most intuitive form of text data. You are given a corpus of complete social media data of tweets. How can you create a model that suggests the hashtags?\n",
    "```python\n",
    "A) Perform Topic Models to obtain most significant words of the corpus\n",
    "B) Train a Bag of Ngrams model to capture top n-grams – words and their combinations\n",
    "C) Train a word2vector model to learn repeating contexts in the sentences\n",
    "D) All of these\n",
    "\n",
    "Solution: (D)\n",
    "\n",
    "All of the techniques can be used to extract most significant terms of a corpus.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
